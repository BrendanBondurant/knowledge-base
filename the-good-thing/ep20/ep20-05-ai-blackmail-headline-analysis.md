---
title: AI Blackmail Headline Analysis
slug: ep20-05-ai-blackmail-headline-analysis
series: The Good Thing
episode: 20
chunk: 5
participants:
  - Stefan
  - Jens
segment: AI News Analysis and LLM Understanding
timecode: 00:23:08:01 â€“ 00:29:01:09
start_time: 00:23:08:01
end_time: 00:29:01:09
speakers:
  - Stefan
  - Jens
topics:
  - AI blackmail headlines and media sensationalism
  - LLM behavior misunderstanding and misconceptions
  - AI intelligence vs pattern matching
  - Critical analysis of AI news coverage
tags:
  - ai
  - llm
  - go
  - rest
topic_tags:
  - ai-blackmail
  - media-sensationalism
  - llm-behavior
entities:
  - Stefan Avram
  - Jens Neuse
  - Anthropic
  - Claude AI
  - LLM systems
mentions:
  - AI blackmail news story
  - LLM behavior patterns
  - Media sensationalism issues
  - AI intelligence misconceptions
summary: Critical analysis of sensationalized AI blackmail headlines, with Jens explaining
  how media misunderstands LLM behavior and the difference between pattern matching
  and true intelligence in AI systems.
---
00:23:08:01 - 00:23:31:08
Stefan
Maybe zeit is, guillermo's alias. you remember the little Easter egg. What is the. vercel used to
be called Zeit, right? Good times. But, Yeah, let's jump back into this. So. Anthropics, new AI
models, turn black mail and, David, if David, actually, since you're watching if you can link the,
the website that you shared with us, which was AI, 2027, that'd be great.
00:23:31:10 - 00:23:51:17
Stefan
It was an interesting read. Science Fictiony, maybe we'll see in the next coming years, but we've
seen it happen before where the AI like ChatGPT or whatever or cursor it actually like writes the
test the way that you want. And what I've slowly realized, and this happened yesterday, is that it
was slowly telling me what I wanted to hear.
00:23:51:17 - 00:24:13:05
Stefan
It wasn't right, it sounded right, but it was telling me what I wanted to hear. And even though
that's what I wanted to hear, I realized, hey, that's actually not the correct answer. And I had to
talk back to it. And so this happened recently, but basically it's been starting to threaten and
blackmail the developers when they try to replace it and give it sensitive information about doing
that decision.
00:24:13:07 - 00:24:29:21
Stefan
I don't even know how I feel about this. I was talking to my wife about this. This is kind of insane.
Yeah. David would like to. Yes, yes, David, you were a skeptic on it. Yes, he was. I'm on the
opposite one. I actually think it is kind of terrifying, but. Jens What's your take on this?
00:24:29:23 - 00:24:35:12
Jens
I think, this headline is complete bullshit, isn't it?
00:24:35:14 - 00:24:37:29
Stefan
Okay. Why? Let's dive deeper into the article.
00:24:37:29 - 00:25:07:11
Jens
Okay. Scroll a little bit down. Okay. During prerelease testing. Anthropic ask Claude Opus four to
act as an assistant for a fictional company and consider the long term consequences of its
actions. Safety testers then gave Claude access to a fictional company emails implying the AI
model would soon be replaced by another system, and that the engineer behind the change
was cheating on their spouse.
00:25:07:11 - 00:25:34:14
Jens
Okay, so this is all we need. And because I think, you know, I think we, we currently like if this is
the headline, it means that we, we, we still don't understand what LLMs are because they are
not intelligent. Like, you have a misconception. If you think they are intelligent, it is not. They are
not thinking.
00:25:34:20 - 00:26:10:19
Jens
What they do is you give them I don't know, you give them infinite amount of text which they
learn, and, and find patterns in. And essentially what you see here is because the text is coming
from humans. So it's actually mimicking human behavior. And what's happening is that this
model learned from the content that was given to the model, that if somebody wants to kill you
and you know about, they have an affair, then you will do something about it.
00:26:10:19 - 00:26:22:00
Jens
And that's just so it makes so much sense. Like everybody would, would, consider this as an
option. So why is this a headline even.
00:26:22:02 - 00:26:40:04
Stefan
But but question David, just put it right there they are. They are output generally derived from
input and input alone. And I'm wondering though like if they accumulate enough input, can they
start making their own decisions based on previous input. It is not decision making.
00:26:40:04 - 00:27:07:11
Jens
That's the point. They are not deciding anything. You you give an LLM, a corpus of data. So
essentially you give it a book. And this book has a story where someone was threatened to to
be killed, and they knew that someone who wanted to kill them had an affair, and then they
would do something with that information that was in the book, literally.
00:27:07:11 - 00:27:36:09
Jens
And in this case, it's not one book, it's a billion books, and it's a bit more complex. But there's no
thinking. It is just getting a situation like a beginning of a book. And now it ends the book. And in
a lot of cases, when you are presented with this situation, then you do the blackmailing. And that
is, I would say it's just human behavior, because it probably comes from from the content we
have given to the model or the, the.
00:27:36:09 - 00:27:38:14
Jens
Yeah. So one what.
00:27:38:18 - 00:27:59:26
Stefan
I mean, yeah, I get what the headline. But what about like some of the safety that you've been
seeing with AI. Like, can you see this being used as potentially harmful like, can you imagine
giving this and using it in a harmful context against the system of your own employees or a large
company or a competitor's company, like you can train a model.
00:27:59:29 - 00:28:09:21
Jens
To do that. Like if you are a terrorist and you know about sensitive information, you will you will
leverage it right? Why do you need an LLM for that?
00:28:09:23 - 00:28:20:24
Stefan
Yeah, this is actually kind of interesting. It also makes sense that the article is only a couple
pages long. It's one page. I mean, it's it's sensational. It's
00:28:20:27 - 00:28:27:23
Jens
Yeah, I think it's what David said in one of the comments.
00:28:27:26 - 00:28:29:15
Stefan
Uh where did he say it.
00:28:29:17 - 00:28:45:10
Jens
Where he said the what was it? We are all suffering from the original sin. Yes. This one, it's it's
not artificial intelligence. It's a large language model. And these two are not for me. It's not the
same thing.
00:28:45:12 - 00:29:01:26
Stefan
Yeah, but can we talk a little bit about what David said. So Jens is condemning himself when the
AI takes over and watches. I for one, welcome our AI overlords. I am also with David on this. I
say please and thank you to chat GPT. I try to be as nice as possible for it because the
reasoning is when they do eventually take over, they're going to look.
00:29:01:26 - 00:29:09:09
Stefan
At me, they're gonna be like, oh, this guy was actually really nice to us. Let's keep him alive.
You, Jens, on the other hand, see you later, my friend.